{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ec1671c",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain-academy/blob/main/module-4/sub-graph.ipynb) [![Open in LangChain Academy](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66e9eba12c7b7688aa3dbb5e_LCA-badge-green.svg)](https://academy.langchain.com/courses/take/intro-to-langgraph/lessons/58239937-lesson-2-sub-graphs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3db85080-2299-4885-a2f6-fffa6a09a238",
   "metadata": {},
   "source": [
    "# Sub-graphs\n",
    "\n",
    "## Review\n",
    "\n",
    "We're building up to a multi-agent research assistant that ties together all of the modules from this course.\n",
    "\n",
    "We just covered parallelization, which is one important LangGraph controllability topic.\n",
    "\n",
    "## Goals\n",
    "\n",
    "Now, we're [going to cover sub-graphs](https://langchain-ai.github.io/langgraph/how-tos/subgraph/#simple-example).\n",
    "\n",
    "## State\n",
    "\n",
    "Sub-graphs allow you to create and manage different states in different parts of your graph. \n",
    "\n",
    "This is particularly useful for multi-agent systems, with teams of agents that each have their own state.\n",
    "\n",
    "Let's consider a toy example:\n",
    "\n",
    "* I have a system that accepts logs\n",
    "* It performs two separate sub-tasks by different agents (summarize logs, find failure modes)\n",
    "* I want to perform these two operations in two different sub-graphs.\n",
    "\n",
    "The most critical thing to understand is how the graphs communicate! \n",
    "\n",
    "In short, communication is **done with over-lapping keys**: \n",
    "\n",
    "* The sub-graphs can access `docs` from the parent\n",
    "* The parent can access `summary/failure_report` from the sub-graphs\n",
    "\n",
    "![subgraph.png](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dbb1abf89f2d847ee6f1ff_sub-graph1.png)\n",
    "\n",
    "## Input\n",
    "\n",
    "Let's define a schema for the logs that will be input to our graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2954e8c6-496f-4394-b56a-681608bf65da",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install -U  langgraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e413ba-e376-4a5f-a666-2d2154aa6fe2",
   "metadata": {},
   "source": [
    "We'll use [LangSmith](https://docs.smith.langchain.com/) for [tracing](https://docs.smith.langchain.com/concepts/tracing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "05b26c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "_set_env(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"langchain-academy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3efaf8bb-f675-4c0b-a575-89c7e2987a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import add\n",
    "from typing_extensions import TypedDict\n",
    "from typing import List, Optional, Annotated\n",
    "\n",
    "# The structure of the logs\n",
    "class Log(TypedDict):\n",
    "    id: str\n",
    "    question: str\n",
    "    docs: Optional[List]\n",
    "    answer: str\n",
    "    grade: Optional[int]\n",
    "    grader: Optional[str]\n",
    "    feedback: Optional[str]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15825627-78c2-4ba0-ad11-95e4afdb771d",
   "metadata": {},
   "source": [
    "## Sub graphs\n",
    "\n",
    "Here is the failure analysis sub-graph, which uses `FailureAnalysisState`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f32986a9-6d11-4646-b2c0-fbae4f524579",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "# Failure Analysis Sub-graph\n",
    "class FailureAnalysisState(TypedDict):\n",
    "    cleaned_logs: List[Log]\n",
    "    failures: List[Log]\n",
    "    fa_summary: str\n",
    "    processed_logs: List[str]\n",
    "\n",
    "class FailureAnalysisOutputState(TypedDict):\n",
    "    fa_summary: str\n",
    "    processed_logs: List[str]\n",
    "\n",
    "def get_failures(state):\n",
    "    \"\"\" Get logs that contain a failure \"\"\"\n",
    "    cleaned_logs = state[\"cleaned_logs\"]\n",
    "    failures = [log for log in cleaned_logs if \"grade\" in log]\n",
    "    return {\"failures\": failures}\n",
    "\n",
    "def generate_summary(state):\n",
    "    \"\"\" Generate summary of failures \"\"\"\n",
    "    failures = state[\"failures\"]\n",
    "    # Add fxn: fa_summary = summarize(failures)\n",
    "    fa_summary = \"Poor quality retrieval of Chroma documentation.\"\n",
    "    return {\"fa_summary\": fa_summary, \"processed_logs\": [f\"failure-analysis-on-log-{failure['id']}\" for failure in failures]}\n",
    "\n",
    "fa_builder = StateGraph(FailureAnalysisState,output=FailureAnalysisOutputState)\n",
    "fa_builder.add_node(\"get_failures\", get_failures)\n",
    "fa_builder.add_node(\"generate_summary\", generate_summary)\n",
    "fa_builder.add_edge(START, \"get_failures\")\n",
    "fa_builder.add_edge(\"get_failures\", \"generate_summary\")\n",
    "fa_builder.add_edge(\"generate_summary\", END)\n",
    "\n",
    "graph = fa_builder.compile()\n",
    "#display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa83f44c-0bb9-48c6-afec-dad536e608fa",
   "metadata": {},
   "source": [
    "Here is the question summarization sub-grap, which uses `QuestionSummarizationState`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7149000c-ffb6-4834-bd9e-d35b36c524e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarization subgraph\n",
    "class QuestionSummarizationState(TypedDict):\n",
    "    cleaned_logs: List[Log]\n",
    "    qs_summary: str\n",
    "    report: str\n",
    "    processed_logs: List[str]\n",
    "\n",
    "class QuestionSummarizationOutputState(TypedDict):\n",
    "    report: str\n",
    "    processed_logs: List[str]\n",
    "\n",
    "def generate_summary(state):\n",
    "    cleaned_logs = state[\"cleaned_logs\"]\n",
    "    # Add fxn: summary = summarize(generate_summary)\n",
    "    summary = \"Questions focused on usage of ChatOllama and Chroma vector store.\"\n",
    "    return {\"qs_summary\": summary, \"processed_logs\": [f\"summary-on-log-{log['id']}\" for log in cleaned_logs]}\n",
    "\n",
    "def send_to_slack(state):\n",
    "    qs_summary = state[\"qs_summary\"]\n",
    "    # Add fxn: report = report_generation(qs_summary)\n",
    "    report = \"foo bar baz\"\n",
    "    return {\"report\": report}\n",
    "\n",
    "qs_builder = StateGraph(QuestionSummarizationState,output=QuestionSummarizationOutputState)\n",
    "qs_builder.add_node(\"generate_summary\", generate_summary)\n",
    "qs_builder.add_node(\"send_to_slack\", send_to_slack)\n",
    "qs_builder.add_edge(START, \"generate_summary\")\n",
    "qs_builder.add_edge(\"generate_summary\", \"send_to_slack\")\n",
    "qs_builder.add_edge(\"send_to_slack\", END)\n",
    "\n",
    "graph = qs_builder.compile()\n",
    "#display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10a5baf-beab-4927-807a-3e6a5ad3d202",
   "metadata": {},
   "source": [
    "## Adding sub graphs to our parent graph\n",
    "\n",
    "Now, we can bring it all together.\n",
    "\n",
    "We create our parent graph with `EntryGraphState`. \n",
    "\n",
    "And we add our sub-graphs as nodes! \n",
    "\n",
    "```\n",
    "entry_builder.add_node(\"question_summarization\", qs_builder.compile())\n",
    "entry_builder.add_node(\"failure_analysis\", fa_builder.compile())\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "587c8fe1-1ae8-411e-a55d-cac299026646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entry Graph\n",
    "class EntryGraphState(TypedDict):\n",
    "    raw_logs: List[Log]\n",
    "    cleaned_logs: Annotated[List[Log], add] # This will be USED BY in BOTH sub-graphs\n",
    "    fa_summary: str # This will only be generated in the FA sub-graph\n",
    "    report: str # This will only be generated in the QS sub-graph\n",
    "    processed_logs:  Annotated[List[int], add] # This will be generated in BOTH sub-graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4da397-310c-4453-969a-e0ae2cc75db8",
   "metadata": {},
   "source": [
    "But, why does `cleaned_logs` have a reducer if it only goes *into* each sub-graph as an input? It is not modified.\n",
    "\n",
    "```\n",
    "cleaned_logs: Annotated[List[Log], add] # This will be USED BY in BOTH sub-graphs\n",
    "```\n",
    "\n",
    "This is because the output state of the subgraphs will contain **all keys**, even if they are unmodified. \n",
    "\n",
    "The sub-graphs are run in parallel.\n",
    "\n",
    "Because the parallel sub-graphs return the same key, it needs to have a reducer like `operator.add` to combine the incoming values from each sub-graph.\n",
    "\n",
    "But, we can work around this by using another concept we talked about before.\n",
    "\n",
    "We can simply create an output state schema for each sub-graph and ensure that the output state schema contains different keys to publish as output.\n",
    "\n",
    "We don't actually need each sub-graph to output `cleaned_logs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "50092b9b-70c1-41b1-a74a-254683e28ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entry Graph\n",
    "class EntryGraphState(TypedDict):\n",
    "    raw_logs: List[Log]\n",
    "    cleaned_logs: List[Log]\n",
    "    fa_summary: str # This will only be generated in the FA sub-graph\n",
    "    report: str # This will only be generated in the QS sub-graph\n",
    "    processed_logs:  Annotated[List[int], add] # This will be generated in BOTH sub-graphs\n",
    "\n",
    "def clean_logs(state):\n",
    "    # Get logs\n",
    "    raw_logs = state[\"raw_logs\"]\n",
    "    # Data cleaning raw_logs -> docs \n",
    "    cleaned_logs = raw_logs\n",
    "    return {\"cleaned_logs\": cleaned_logs}\n",
    "\n",
    "entry_builder = StateGraph(EntryGraphState)\n",
    "entry_builder.add_node(\"clean_logs\", clean_logs)\n",
    "entry_builder.add_node(\"question_summarization\", qs_builder.compile())\n",
    "entry_builder.add_node(\"failure_analysis\", fa_builder.compile())\n",
    "\n",
    "entry_builder.add_edge(START, \"clean_logs\")\n",
    "entry_builder.add_edge(\"clean_logs\", \"failure_analysis\")\n",
    "entry_builder.add_edge(\"clean_logs\", \"question_summarization\")\n",
    "entry_builder.add_edge(\"failure_analysis\", END)\n",
    "entry_builder.add_edge(\"question_summarization\", END)\n",
    "\n",
    "graph = entry_builder.compile()\n",
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# Setting xray to 1 will show the internal structure of the nested graph\n",
    "#display(Image(graph.get_graph(xray=1).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "17af1254-4e75-4349-9a79-295f4ec95016",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'raw_logs': [{'id': '1',\n",
       "   'question': 'How can I import ChatOllama?',\n",
       "   'answer': \"To import ChatOllama, use: 'from langchain_community.chat_models import ChatOllama.'\"},\n",
       "  {'id': '2',\n",
       "   'question': 'How can I use Chroma vector store?',\n",
       "   'answer': 'To use Chroma, define: rag_chain = create_retrieval_chain(retriever, question_answer_chain).',\n",
       "   'grade': 0,\n",
       "   'grader': 'Document Relevance Recall',\n",
       "   'feedback': 'The retrieved documents discuss vector stores in general, but not Chroma specifically'}],\n",
       " 'cleaned_logs': [{'id': '1',\n",
       "   'question': 'How can I import ChatOllama?',\n",
       "   'answer': \"To import ChatOllama, use: 'from langchain_community.chat_models import ChatOllama.'\"},\n",
       "  {'id': '2',\n",
       "   'question': 'How can I use Chroma vector store?',\n",
       "   'answer': 'To use Chroma, define: rag_chain = create_retrieval_chain(retriever, question_answer_chain).',\n",
       "   'grade': 0,\n",
       "   'grader': 'Document Relevance Recall',\n",
       "   'feedback': 'The retrieved documents discuss vector stores in general, but not Chroma specifically'}],\n",
       " 'fa_summary': 'Poor quality retrieval of Chroma documentation.',\n",
       " 'report': 'foo bar baz',\n",
       " 'processed_logs': ['failure-analysis-on-log-2',\n",
       "  'summary-on-log-1',\n",
       "  'summary-on-log-2']}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dummy logs\n",
    "question_answer = Log(\n",
    "    id=\"1\",\n",
    "    question=\"How can I import ChatOllama?\",\n",
    "    answer=\"To import ChatOllama, use: 'from langchain_community.chat_models import ChatOllama.'\",\n",
    ")\n",
    "\n",
    "question_answer_feedback = Log(\n",
    "    id=\"2\",\n",
    "    question=\"How can I use Chroma vector store?\",\n",
    "    answer=\"To use Chroma, define: rag_chain = create_retrieval_chain(retriever, question_answer_chain).\",\n",
    "    grade=0,\n",
    "    grader=\"Document Relevance Recall\",\n",
    "    feedback=\"The retrieved documents discuss vector stores in general, but not Chroma specifically\",\n",
    ")\n",
    "\n",
    "raw_logs = [question_answer,question_answer_feedback]\n",
    "graph.invoke({\"raw_logs\": raw_logs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e241ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9192d228-4d3d-4fb0-8bea-26772c3d2e0b",
   "metadata": {},
   "source": [
    "## LangSmith\n",
    "\n",
    "Let's look at the LangSmith trace:\n",
    "\n",
    "https://smith.langchain.com/public/f8f86f61-1b30-48cf-b055-3734dfceadf2/r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e836d4",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lc-academy-env (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
